import os
import re
import json
import argparse
import shutil
from collections import defaultdict


# æ“ä½œå®ä¾‹
# python ./trans.py --base /Users/mi/Documents/miwear/MiJiaWear/Resource --target /Users/mi/Documents/fitness-rn/app/assets/sport/res/localizedString --a



# ========== å‚æ•°è§£æ ==========
parser = argparse.ArgumentParser()
parser.add_argument("--base", required=True, help="åŸå§‹ .lproj èµ„æºç›®å½•")
parser.add_argument("--target", required=False, help="è¿½åŠ  JSON çš„ç›®æ ‡ç›®å½•")
parser.add_argument("--a", action="store_true", help="æ˜¯å¦å°†ç»“æœè¿½åŠ åˆ°ç›®æ ‡ JSON")
args = parser.parse_args()

base_dir = args.base
target_append_dir = args.target
should_append = args.a

source_locale = "zh-Hans.lproj"
output_dir = os.path.join(os.getcwd(), "lproj-json")
log_output_path = os.path.join(output_dir, "translation_log.txt")

if os.path.exists(log_output_path):
    print(f"æ£€æµ‹åˆ°ä¹‹å‰çš„æ—¥å¿—æ–‡ä»¶ {log_output_path}ï¼Œå¼€å§‹åˆ é™¤æ•´ä¸ª {output_dir} æ–‡ä»¶å¤¹...")
    shutil.rmtree(output_dir)
    print(f"å·²åˆ é™¤ {output_dir} æ–‡ä»¶å¤¹ï¼Œå‡†å¤‡é‡æ–°ç”Ÿæˆå†…å®¹")



# ===== æ˜ å°„è¡¨è§„åˆ™è¯´æ˜ =====
# - è‹¥ "xx-YY:xx" è¡¨ç¤ºå°†åœ°åŒºå˜ç§ï¼ˆå¦‚ ne-INï¼‰æ˜ å°„åˆ°é€šç”¨è¯­è¨€ï¼ˆå¦‚ neï¼‰
# - è‹¥ "xx" æ²¡æœ‰å†’å·ï¼Œè¡¨ç¤ºåŸæ ·è¾“å‡ºä¸º xx.json
# - æœªåœ¨æ­¤åˆ—å‡ºçš„è¯­è¨€å°†ä¼šè¢«è·³è¿‡ï¼Œä¸ä¼šç”Ÿæˆ JSON
# å¤šå¯¹ä¸€è¯´æ˜ï¼š
# zh-Hant-HK:zh-HK
#   zh-Hant:zh-HK åªä¼šæœ‰ç¬¬ä¸€ä¸ªå‡ºç°çš„ç›®å½•è¢«å¤„ç†å†™å…¥ zh-HK.json



# ===== æ”¯æŒæ³¨é‡Šçš„æ˜ å°„è¡¨ =====
mapping_raw_lines = [
    "ar",
    "as",
    "az",
    "be",
    "bg-BG:bg",   # ä¿åŠ åˆ©äºšè¯­
    "bn-IN:bn",   # å­ŸåŠ æ‹‰è¯­ï¼ˆå°åº¦ï¼‰
    "bn",
    #"bo",  #è—è¯­ï¼ŒIOSæ²¡æœ‰è—è¯­ï¼Œä¸å¤„ç†
    "bs",
    "ca",
    "cs",
    "da",
    "de",         # å¾·è¯­
    "el",
    "en-AU",      # è‹±è¯­ï¼ˆæ¾³æ´²ï¼‰
    "en-GB",      # è‹±è¯­ï¼ˆè‹±å›½ï¼‰
    "en-IN",      # è‹±è¯­ï¼ˆå°åº¦ï¼‰
    "en:en-US",   # é»˜è®¤è‹±è¯­æ˜ å°„åˆ°ç¾å›½è‹±è¯­
    "es",
    "et",
    "eu",
    "fa",
    "fi",
    "fr",
    "gl",
    "gu-IN:gu",
    "ha",
    "he",
    "hi",
    "hr",
    "hu",
    "hy-AM:hy",
    "id",
    "is",
    "it",
    "ja",
    "ka",
    "kk",
    "km",
    "kn",
    "ko",
    "lo",
    "lt",
    "lv",
    "mk",
    "ml-IN:ml",
    "mr",
    "ms",
    "mt",
    "my",
    "nb",
    "ne-IN:ne",     # å°¼æ³Šå°”è¯­ï¼ˆå°åº¦ï¼‰
    "ne-NP:ne",     # å°¼æ³Šå°”è¯­ï¼ˆå°¼æ³Šå°”ï¼‰
    "nl",
    "or",
    "pa-IN:pa",
    "pl",
    "pt-BR",
    "pt-PT",
    "ro",
    "ru",
    "sk",
    "sl",
    "sq",
    "sr",
    "sv",
    "sw",
    "ta",
    "te",
    "th",
    "tr",
    #"ug", #ç»´å¾å°”è¯­ï¼ŒIOSæ²¡æœ‰ç»´å¾å°”è¯­ ä¸å¤„ç†
    "uk",
    "ur-IN",
    "ur-PK",
    "uz",
    "vi",
    "zh-Hans:zh-CN",     # ç®€ä½“ä¸­æ–‡
    "zh-Hant-HK:zh-HK",  # ç¹ä½“ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰
    "zh-Hant:zh-HK",     # ä¹ŸæŒ‡å‘é¦™æ¸¯
    "zh-Hant:zh-TW",     # å¯åˆ‡æ¢ä¸ºå°æ¹¾ï¼ˆæ³¨é‡Šå…¶ä¸­ä¸€ä¸ªå³å¯ï¼‰
]

# ===== Key æ˜ å°„ =====
key_override_map = {
    "è¿åŠ¨": "sport.tabName",
    "æˆ·å¤–è·‘æ­¥": "sport.entry.outdoorRunning",
    "å¥èµ°": "sport.entry.outsideWalking",
    "æˆ·å¤–éª‘è¡Œ": "sport.entry.outsideRiding",
    "å®¤å†…è·‘æ­¥": "sport.entry.indoorRunning",
    "è·³ç»³": "sport.entry.ropeSkipping",
    "è¿åŠ¨è®°å½•": "sport.plan.sportRecord",
    "å…¨éƒ¨": "all.title",
    "æš‚æ— è¿åŠ¨è®°å½•": "sport.plan.noRecord",
    "è®­ç»ƒæŒ‡æ ‡": "sport.plan.trainingIndicator",
    "æ‰«ä¸€æ‰«": "sport.plan.scanQRCode",
    "æ·»åŠ è®¾å¤‡": "sport.plan.addDevice",
    "åŠ¨æ„Ÿå•è½¦": "sport.entry.indoorBicycle"
}
target_values = list(key_override_map.keys())

def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def parse_strings_file(filepath):
    translations = {}
    if not os.path.exists(filepath):
        return translations
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
        matches = re.findall(r'"(.*?)"\s*=\s*"(.+?)";', content)
        for key, value in matches:
            translations[key] = value
    return translations

# ===== è§£ææ˜ å°„è¡¨ï¼ˆå¸¦æ³¨é‡Šï¼‰=====
mapping = {}
for line in mapping_raw_lines:
    line = line.strip()
    if not line or line.startswith("#"):
        continue
    if "#" in line:
        line = line.split("#", 1)[0].strip()
    if ":" in line:
        k, v = line.split(":", 1)
        mapping[k.strip()] = v.strip()
    else:
        mapping[line] = line

# ===== åˆå§‹åŒ– =====
ensure_dir(output_dir)
log_lines = []

def log(msg):
    print(msg)
    log_lines.append(msg)

# ===== Step 1: è·å– zh-Hans.lproj çš„åŸå§‹é”®å€¼è¡¨ =====
source_file = os.path.join(base_dir, source_locale, "Localizable.strings")
source_map = parse_strings_file(source_file)
reverse_source_map = defaultdict(list)
for k, v in source_map.items():
    reverse_source_map[v].append(k)

value_to_key_map = {}
initial_key_list = []
log("\nğŸ” Step 1: ä» zh-Hans.lproj åå‘æŸ¥æ‰¾ç›®æ ‡valueå¯¹åº”çš„keyå’Œå¤‡é€‰key")
for val in target_values:
    keys = reverse_source_map.get(val)
    if keys:
        main_key = keys[0]
        fallback_keys = keys[1:]
        initial_key_list.append(main_key)
        value_to_key_map[main_key] = {"value": val, "fallback_keys": fallback_keys}
        log(f"  âœ… \"{val}\" => ä¸»key: {main_key}ï¼Œå¤‡é€‰key: {fallback_keys}")
    else:
        log(f"  âŒ \"{val}\" æœªæ‰¾åˆ°å¯¹åº”key")

# ===== Step 2: éå†å¤šè¯­è¨€åŒ…å¹¶è¾“å‡º JSON =====
translations_all = defaultdict(dict)
missing_keys_log = defaultdict(list)
processed_json_targets = set()
lproj_folders = [d for d in os.listdir(base_dir) if d.endswith('.lproj') and d != source_locale]

for folder in sorted(lproj_folders):
    locale_code = folder[:-6]
    if locale_code not in mapping:
        continue  # âŒ ä¸åœ¨ mapping è¡¨ä¸­çš„è¯­è¨€ï¼Œä¸å¤„ç†

    json_target = mapping[locale_code]
    if json_target in processed_json_targets:
        continue

    lproj_path = os.path.join(base_dir, folder, "Localizable.strings")
    if not os.path.exists(lproj_path):
        continue

    locale_map = parse_strings_file(lproj_path)
    output_json_dict = {}

    for orig_key in initial_key_list:
        val = value_to_key_map[orig_key]["value"]
        fallback_keys = value_to_key_map[orig_key]["fallback_keys"]
        out_key = key_override_map.get(val, orig_key)
        search_keys = [orig_key] + fallback_keys
        trans_val = None
        for k in search_keys:
            if k in locale_map:
                trans_val = locale_map[k]
                break
        if trans_val:
            output_json_dict[out_key] = trans_val
        else:
            missing_keys_log[folder].append(orig_key)

    out_path = os.path.join(output_dir, f"{json_target}.json")
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(output_json_dict, f, ensure_ascii=False, indent=2)
    processed_json_targets.add(json_target)

# ===== Step 3: å¯é€‰è¿½åŠ åˆ°å·²æœ‰ JSON æ–‡ä»¶ =====
if should_append and target_append_dir:
    def append_to_existing_json(target_append_dir, output_dir):
        for filename in os.listdir(output_dir):
            if not filename.endswith(".json"):
                continue
            src_path = os.path.join(output_dir, filename)
            tgt_path = os.path.join(target_append_dir, filename)

            with open(src_path, 'r', encoding='utf-8') as f:
                new_data = json.load(f)

            if os.path.exists(tgt_path):
                with open(tgt_path, 'r', encoding='utf-8') as f:
                    old_lines = f.readlines()

                try:
                    existing_data = json.loads("".join(old_lines))
                except json.JSONDecodeError:
                    msg = f"âŒ æ— æ³•è§£æ {tgt_path}ï¼Œè·³è¿‡"
                    print(msg)
                    log_lines.append(msg)
                    continue

                to_add = {k: v for k, v in new_data.items() if k not in existing_data}
                existing_keys = [k for k in new_data.keys() if k in existing_data]

                if not to_add:
                    msg = f"â„¹ï¸ {tgt_path} ä¸­å·²åŒ…å«å…¨éƒ¨è¯æ¡ï¼Œæ— éœ€è¿½åŠ "
                    print(msg)
                    log_lines.append(msg)
                    continue

                first_brace_index = next((i for i, l in enumerate(old_lines) if l.strip() == '{'), None)
                last_kv_index = next((i for i in reversed(range(len(old_lines))) if old_lines[i].strip() and not old_lines[i].strip().startswith("//") and old_lines[i].strip() not in ("{", "}")), None)

                insertion = [f'  "{k}": "{v}"{"," if idx < len(to_add) - 1 else ""}\n' for idx, (k, v) in enumerate(to_add.items())]

                if last_kv_index is not None and first_brace_index is not None and last_kv_index > first_brace_index:
                    if not old_lines[last_kv_index].rstrip().endswith(","):
                        old_lines[last_kv_index] = old_lines[last_kv_index].rstrip() + ",\n"
                    new_lines = old_lines[:last_kv_index + 1] + insertion + old_lines[last_kv_index + 1:]
                else:
                    new_lines = old_lines[:first_brace_index + 1] + insertion + old_lines[first_brace_index + 1:]

                with open(tgt_path, 'w', encoding='utf-8') as f:
                    f.writelines(new_lines)

                msg = f"âœ… å·²è¿½åŠ  {len(to_add)} é¡¹åˆ° {tgt_path}"
                if existing_keys:
                    msg += f"ï¼Œå·²æœ‰ {len(existing_keys)} é¡¹è¯æ¡æœªè¿½åŠ : {existing_keys}"
                print(msg)
                log_lines.append(msg)
            else:
                with open(tgt_path, 'w', encoding='utf-8') as f:
                    json.dump(new_data, f, ensure_ascii=False, indent=2)
                msg = f"ğŸ†• åˆ›å»ºå¹¶å†™å…¥æ–°æ–‡ä»¶ {tgt_path}"
                print(msg)
                log_lines.append(msg)

    append_to_existing_json(target_append_dir, output_dir)

# ===== Step 4: å†™å…¥æ—¥å¿— =====
ensure_dir(output_dir)
with open(log_output_path, 'w', encoding='utf-8') as logf:
    logf.write("\n".join(log_lines))

print(f"\næ—¥å¿—å†™å…¥å®Œæˆ: {log_output_path}")
